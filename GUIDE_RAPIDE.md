# Guide Rapide - Projet Neural Networks

## ğŸ“‹ Description du Projet

**Projet**: Reconnaissance de caractÃ¨res alphanumÃ©riques par Perceptron Multicouche (MLP)

**Objectif**: DÃ©velopper un MLP pour reconnaÃ®tre des chiffres (0-9) et des lettres (A-Z) Ã  partir d'images

**Datasets utilisÃ©s**: 
- MNIST (chiffres manuscrits)
- EMNIST Letters (lettres manuscrites)

## âœ… Ce qui a Ã©tÃ© fait

1. **Nettoyage du code** - Correction de tous les problÃ¨mes de compatibilitÃ©
2. **Simplification** - Notebook structurÃ© et facile Ã  suivre
3. **Optimisation** - RÃ©duction du temps d'entraÃ®nement (50 epochs au lieu de 196)
4. **Documentation** - Commentaires en franÃ§ais et explications claires

## ğŸš€ Installation Rapide

### 1. Installer les dÃ©pendances

```bash
pip install tensorflow keras scikit-learn scipy matplotlib opencv-python
```

### 2. VÃ©rifier que les donnÃ©es sont prÃ©sentes

Assurez-vous que le fichier `data/emnist-letters.mat` existe dans votre rÃ©pertoire.

## ğŸ“ Fichiers Importants

- **`mnist_emnist_classification_clean.ipynb`** â† LE FICHIER Ã€ UTILISER
- `image-classification-mnist-emist.ipynb` (ancien fichier, peut Ãªtre ignorÃ©)
- `data/emnist-letters.mat` (donnÃ©es EMNIST)

## ğŸ¯ Comment ExÃ©cuter

### Option 1: Tout ExÃ©cuter (RecommandÃ©)
1. Ouvrir `mnist_emnist_classification_clean.ipynb`
2. Cliquer sur "Run All" ou exÃ©cuter cellule par cellule
3. Attendre l'entraÃ®nement (15-30 minutes selon votre machine)

### Option 2: MNIST Seulement (Plus Rapide - 5-10 minutes)
1. ExÃ©cuter les sections 1 et 2 uniquement
2. Ignorer la section 3 (EMNIST)
3. Vous obtiendrez ~99% de prÃ©cision sur les chiffres

### Option 3: Utiliser les ModÃ¨les PrÃ©-entraÃ®nÃ©s
Si les fichiers `.h5` existent dÃ©jÃ :
```python
from tensorflow.keras.models import load_model
model = load_model('mnist_v13.h5')  # ou emnist_v5.h5
```

## ğŸ“Š RÃ©sultats Attendus

### MNIST (Chiffres 0-9)
- **Training accuracy**: ~99.9%
- **Validation accuracy**: ~99.6%
- **Test accuracy**: ~99.5%

### EMNIST Letters (Lettres A-Z)
- **Training accuracy**: ~98.7%
- **Validation accuracy**: ~95.1%
- **Test accuracy**: ~94.9%

## ğŸ—ï¸ Architecture du RÃ©seau

```
Input (28x28x1)
    â†“
Conv2D (32 filtres, 3x3) + ReLU + BatchNorm + MaxPooling
    â†“
Conv2D (64 filtres, 3x3) + ReLU + BatchNorm + MaxPooling + Dropout(0.5)
    â†“
Flatten
    â†“
Dense(1568) + ReLU + BatchNorm + Dropout(0.5)
    â†“
Dense(1000) + ReLU + BatchNorm + Dropout(0.5)
    â†“
Dense(700) + ReLU + BatchNorm + Dropout(0.5)
    â†“
Dense(300) + ReLU + BatchNorm + Dropout(0.5)
    â†“
Dense(100) + ReLU + BatchNorm
    â†“
Dense(10 ou 27) + Softmax
```

**Total**: ~5-6 millions de paramÃ¨tres

## ğŸ”§ Techniques UtilisÃ©es

1. **Batch Normalization** - Ã‰vite le problÃ¨me de vanishing gradient
2. **Dropout (0.5)** - RÃ©duit l'overfitting
3. **Adadelta Optimizer** - Learning rate adaptatif automatique
4. **Early Stopping** - ArrÃªt automatique si pas d'amÃ©lioration
5. **Data Split** - Train (83.3%) / Validation (16.7%) / Test

## â±ï¸ Temps d'ExÃ©cution

- **MNIST seul**: 5-10 minutes
- **EMNIST seul**: 10-20 minutes
- **Les deux**: 20-30 minutes

*Note: Temps avec CPU. Avec GPU, c'est 5-10x plus rapide*

## ğŸ“ˆ Visualisations Incluses

- Courbes d'accuracy (train vs validation)
- Courbes de loss
- Exemples d'erreurs de classification
- Exemples de prÃ©dictions correctes avec confiance

## ğŸ“ Pour la PrÃ©sentation

### Points ClÃ©s Ã  Mentionner:

1. **Architecture MLP profonde** avec 5 couches fully connected
2. **Techniques modernes**: Batch Normalization, Dropout, Early Stopping
3. **RÃ©sultats excellents**: 99.5% sur MNIST, 95% sur EMNIST
4. **GÃ©nÃ©ralisation**: Validation/test splits pour Ã©viter l'overfitting
5. **Optimizer intelligent**: Adadelta (pas besoin de tuner le learning rate)

### DÃ©fis RÃ©solus:

- **Vanishing gradient**: RÃ©solu avec Batch Normalization
- **Overfitting**: RÃ©solu avec Dropout et rÃ©gularisation
- **Learning rate**: RÃ©solu avec Adadelta optimizer
- **Dead ReLU**: MinimisÃ© avec Batch Normalization

## ğŸ› ProblÃ¨mes Courants

### Erreur: "No module named tensorflow"
```bash
pip install tensorflow
```

### Erreur: "File not found: emnist-letters.mat"
VÃ©rifiez que le fichier est dans le dossier `data/`

### Le modÃ¨le n'apprend pas bien
- VÃ©rifiez que les donnÃ©es sont normalisÃ©es (division par 255)
- Augmentez le nombre d'epochs si nÃ©cessaire

## ğŸ“ Aide Rapide

Si vous avez des problÃ¨mes:
1. VÃ©rifiez que toutes les bibliothÃ¨ques sont installÃ©es
2. VÃ©rifiez que les donnÃ©es EMNIST sont prÃ©sentes
3. Utilisez MNIST seul pour tester rapidement
4. RÃ©duisez les epochs Ã  10 pour un test rapide

## ğŸ¯ Version Minimale (Si Vraiment PressÃ©)

Si vous manquez vraiment de temps, utilisez seulement la **Section 2 (MNIST)**:
- Temps: 5-10 minutes
- RÃ©sultat: 99% de prÃ©cision
- Suffit largement pour le projet!

---

**Bon courage! ğŸš€**
