\documentclass[fleqn,10pt]{olplainarticle}

\title{Classification de Caractères Alphanumériques par Perceptron Multicouche (MLP)}

\author[1]{Oussama Ammari}
\author[2]{Mohamed Amine Ben Mosbah}

\affil[1]{IA3.1, ENISo}

\keywords{Perceptron Multicouche, MLP, Deep Learning, MNIST, EMNIST, Reconnaissance de Caractères, Batch Normalization, Dropout, Adam Optimizer}

\begin{abstract}
Ce projet présente le développement et l'implémentation d'un réseau de neurones de type Perceptron Multicouche (MLP) pour la reconnaissance automatique de caractères manuscrits alphanumériques. Le système développé traite deux ensembles de données de référence : MNIST pour les chiffres manuscrits (0-9) et EMNIST Letters pour les lettres majuscules (A-Z). L'architecture MLP proposée comprend trois couches cachées fully connected avec respectivement 512, 256 et 128 neurones, utilisant l'optimiseur Adam avec learning rate de 0.001 et des techniques de régularisation avancées incluant Batch Normalization et Dropout (0.3). Le prétraitement des données comprend la normalisation des pixels vers l'intervalle [0, 1], l'aplatissement des images 28×28 en vecteurs de 784 éléments, et l'encodage one-hot des labels. Les résultats expérimentaux démontrent une précision de 98.42\% sur l'ensemble de test MNIST (158 erreurs sur 10,000 images) et 92.45\% sur EMNIST Letters (1,117 erreurs sur 14,800 images), validant l'efficacité de l'approche MLP pour des tâches de reconnaissance de caractères.
\end{abstract}

\begin{document}

\flushbottom
\maketitle
\thispagestyle{empty}

\section*{Introduction}

La reconnaissance automatique de caractères manuscrits constitue un problème fondamental en vision par ordinateur et en apprentissage automatique depuis plusieurs décennies. Cette tâche trouve des applications pratiques majeures dans de nombreux domaines : numérisation automatique de documents, reconnaissance d'adresses postales pour le tri automatique du courrier, systèmes de saisie manuscrite pour tablettes et smartphones, lecture automatique de formulaires administratifs, et assistance aux malvoyants.

Le Perceptron Multicouche (MLP), également appelé réseau de neurones fully connected ou dense, représente l'architecture la plus fondamentale des réseaux de neurones artificiels. Contrairement aux architectures convolutionnelles (CNN) qui exploitent explicitement la structure spatiale bidimensionnelle des images, le MLP traite les données sous forme de vecteurs unidimensionnels obtenus par aplatissement (flattening) des images. Cette approche, bien que conceptuellement plus simple, nécessite l'apprentissage d'un nombre important de paramètres et perd l'information de localité spatiale.

Malgré ces limitations théoriques, les MLP restent pertinents pour plusieurs raisons. Premièrement, leur simplicité architecturale les rend plus faciles à comprendre et à implémenter, ce qui en fait d'excellents outils pédagogiques pour l'apprentissage des concepts fondamentaux du deep learning. Deuxièmement, avec des techniques de régularisation modernes comme Batch Normalization et Dropout, combinées à des optimiseurs adaptatifs comme Adam, les MLP peuvent atteindre des performances remarquables sur des images de faible résolution. Troisièmement, leur temps d'entraînement et d'inférence est généralement plus rapide que celui des CNN pour des architectures de complexité comparable.

L'objectif de ce projet est de développer un système de reconnaissance de caractères alphanumériques basé sur une architecture MLP optimisée. Le système doit être capable de classifier automatiquement des chiffres manuscrits (0-9) issus du dataset MNIST et des lettres majuscules manuscrites (A-Z) issues du dataset EMNIST Letters. Ces deux datasets constituent des benchmarks standards dans la communauté du machine learning, permettant une comparaison directe avec l'état de l'art.

Pour atteindre cet objectif, nous adoptons une méthodologie rigoureuse incluant : (1) un prétraitement soigné des données pour normaliser et formater correctement les images, (2) la conception d'une architecture MLP à trois couches cachées de tailles progressives (512, 256, 128 neurones), (3) l'utilisation de techniques de régularisation avancées pour éviter le surapprentissage, (4) l'optimisation avec Adam pour garantir une convergence rapide, et (5) une évaluation détaillée des performances avec analyse des erreurs.

Ce rapport présente l'ensemble du travail réalisé, depuis la préparation des données jusqu'à l'évaluation finale des modèles, en passant par la justification des choix architecturaux et l'analyse des résultats obtenus.

\section*{Méthodes et Matériaux}

\subsection*{Ensembles de Données}

\textbf{MNIST (Modified National Institute of Standards and Technology)} constitue le dataset de référence pour la reconnaissance de chiffres manuscrits depuis sa publication en 1998. Il contient 70,000 images en niveaux de gris de dimensions 28×28 pixels, représentant les chiffres de 0 à 9. L'ensemble d'entraînement comprend 60,000 images et l'ensemble de test 10,000 images. Les images ont été prétraitées pour être centrées et normalisées, avec les chiffres occupant approximativement la même région de l'image. Ce dataset est directement accessible via l'API TensorFlow/Keras, ce qui facilite son chargement et son utilisation.

Pour notre expérimentation, nous avons appliqué un partitionnement stratégique de l'ensemble d'entraînement original. Les 60,000 images d'entraînement ont été divisées en deux sous-ensembles : un ensemble d'entraînement de 50,000 images (83.3\%) et un ensemble de validation de 10,000 images (16.7\%). Cette séparation train/validation permet de surveiller les performances du modèle pendant l'entraînement et de détecter le surapprentissage. L'ensemble de test original de 10,000 images reste intact et sert uniquement pour l'évaluation finale du modèle, garantissant ainsi une mesure objective des performances sur des données jamais vues pendant l'entraînement.

Le code de chargement et de partitionnement est le suivant :

\begin{verbatim}
from tensorflow.keras.datasets import mnist
from sklearn.model_selection import train_test_split

(x_train, y_train), (x_test, y_test) = mnist.load_data()

x_train, x_valid, y_train, y_valid = train_test_split(
    x_train, y_train, 
    test_size=0.167, 
    random_state=42, 
    shuffle=True
)
\end{verbatim}

\textbf{EMNIST Letters (Extended MNIST)} représente une extension ambitieuse du dataset MNIST, incluant non seulement des chiffres mais également des lettres manuscrites. La variante "Letters" que nous utilisons se concentre exclusivement sur les 26 lettres majuscules de l'alphabet latin (A-Z). Le dataset contient approximativement 145,600 images d'entraînement et 14,800 images de test, toutes au format 28×28 pixels en niveaux de gris, identique à MNIST.

Une particularité technique importante d'EMNIST est que les données sont distribuées au format MATLAB (.mat), nécessitant l'utilisation de la bibliothèque scipy pour le chargement. De plus, les images sont stockées selon l'ordre Fortran (column-major) plutôt que l'ordre C (row-major) standard en Python, ce qui nécessite une attention particulière lors du reshape pour éviter une corruption de l'orientation des images.

Le dataset EMNIST Letters utilise 27 classes numérotées de 0 à 26, où la classe 0 n'est pas utilisée et les classes 1-26 correspondent respectivement aux lettres A-Z. Cette convention doit être prise en compte lors du mapping des prédictions vers les caractères alphabétiques.

Le code de chargement et de correction d'orientation pour EMNIST est :

\begin{verbatim}
from scipy import io as sio

mat = sio.loadmat('data/emnist-letters.mat')
data = mat['dataset']

X_train = data['train'][0,0]['images'][0,0]
y_train = data['train'][0,0]['labels'][0,0]
X_test = data['test'][0,0]['images'][0,0]
y_test = data['test'][0,0]['labels'][0,0]

# Reshape avec Fortran ordering (MATLAB utilise column-major)
x_train = X_train.reshape((X_train.shape[0], 28, 28), order='F')
y_train = y_train.reshape(-1)
x_test = X_test.reshape((X_test.shape[0], 28, 28), order='F')
y_test = y_test.reshape(-1)

# Partitionnement train/validation identique à MNIST
x_train, x_valid, y_train, y_valid = train_test_split(
    x_train, y_train, 
    test_size=0.167, 
    random_state=0, 
    shuffle=True
)
\end{verbatim}

Le choix de ces deux datasets se justifie par plusieurs facteurs. D'abord, MNIST constitue un standard académique incontournable, permettant la comparaison directe avec des milliers de publications de recherche. Ensuite, EMNIST Letters représente une extension logique permettant de valider la généralisation de notre architecture MLP sur un problème plus complexe (26 classes vs 10). Enfin, les deux datasets partagent le même format d'image (28×28 grayscale), ce qui permet de réutiliser exactement la même architecture et le même pipeline de prétraitement, facilitant ainsi la comparaison des performances et l'analyse des différences.

\subsection*{Prétraitement des Données}

Le prétraitement des données constitue une étape critique qui influence directement les performances et la vitesse de convergence du modèle. Notre pipeline de prétraitement comprend quatre étapes principales, appliquées de manière identique aux datasets MNIST et EMNIST.

\textbf{Étape 1 : Aplatissement (Flattening)}. Contrairement aux architectures convolutionnelles qui préservent la structure bidimensionnelle des images, les MLP nécessitent des vecteurs unidimensionnels en entrée. Nous transformons donc chaque image 28×28 pixels en un vecteur de 784 éléments (28×28=784). Cette opération est réalisée par la couche \texttt{Flatten} de Keras, qui sera la première couche de notre modèle :

$$
\text{Image}_{28 \times 28} \rightarrow \text{Vecteur}_{784}
$$

Cette transformation préserve toutes les valeurs de pixels mais perd l'information de voisinage spatial. Par exemple, deux pixels adjacents dans l'image originale deviennent simplement deux éléments consécutifs dans le vecteur, sans que le réseau ait connaissance de leur proximité spatiale originale.

\textbf{Étape 2 : Normalisation}. Les valeurs brutes des pixels sont des entiers dans l'intervalle [0, 255], où 0 représente le noir et 255 le blanc. Cette échelle pose deux problèmes majeurs pour l'entraînement des réseaux de neurones. Premièrement, les grandes valeurs numériques peuvent causer des gradients instables pendant la rétropropagation, rendant l'optimisation difficile. Deuxièmement, la plupart des fonctions d'activation (notamment ReLU et sigmoid) sont conçues pour fonctionner optimalement avec des valeurs d'entrée proches de zéro.

Nous appliquons donc une normalisation Min-Max simple, divisant toutes les valeurs de pixels par 255 pour les ramener dans l'intervalle [0, 1] :

$$
X_{\text{norm}} = \frac{X_{\text{raw}}}{255} = \frac{X_{\text{raw}} - 0}{255 - 0}
$$

Cette normalisation est appliquée uniformément à tous les ensembles (train, validation, test) :

\begin{verbatim}
x_train = x_train.astype('float32') / 255.0
x_valid = x_valid.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0
\end{verbatim}

La conversion en \texttt{float32} avant la division est importante pour éviter la division entière et garantir la précision numérique.

\textbf{Étape 3 : Reshape pour compatibilité}. Bien que notre MLP traite des vecteurs 1D, Keras attend un format spécifique (batch\_size, height, width, channels) pour la couche Flatten. Nous devons donc ajouter une dimension de canal aux images :

$$
(N, 28, 28) \rightarrow (N, 28, 28, 1)
$$

où $N$ est le nombre d'images et 1 représente un seul canal (grayscale).

\begin{verbatim}
x_train = x_train.reshape(-1, 28, 28, 1)
x_valid = x_valid.reshape(-1, 28, 28, 1)
x_test = x_test.reshape(-1, 28, 28, 1)
\end{verbatim}

\textbf{Étape 4 : Encodage One-Hot des labels}. Les labels originaux sont des entiers (0-9 pour MNIST, 1-26 pour EMNIST Letters). Pour une classification multi-classe avec l'entropie croisée catégorielle, nous devons convertir ces labels en vecteurs binaires one-hot.

Pour MNIST (10 classes), chaque label devient un vecteur de 10 éléments avec un seul 1 et neuf 0 :

$$
\text{Label } 3 \rightarrow [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
$$

Pour EMNIST Letters (27 classes, dont classe 0 inutilisée), chaque label devient un vecteur de 27 éléments :

$$
\text{Label } 1 \text{ (A)} \rightarrow [0, 1, 0, 0, ..., 0] \quad (27 \text{ éléments})
$$

Cette transformation est réalisée avec la fonction \texttt{to\_categorical} de Keras :

\begin{verbatim}
from tensorflow.keras.utils import to_categorical

# Pour MNIST
y_train = to_categorical(y_train, 10)
y_valid = to_categorical(y_valid, 10)
y_test = to_categorical(y_test, 10)

# Pour EMNIST
y_train = to_categorical(y_train, 27)
y_valid = to_categorical(y_valid, 27)
y_test = to_categorical(y_test, 27)
\end{verbatim}

L'encodage one-hot présente plusieurs avantages : il évite d'introduire une ordinalité artificielle entre les classes (par exemple, le fait que '9' > '1' numériquement n'a aucune signification sémantique), il facilite le calcul de l'entropie croisée catégorielle, et il permet au réseau de traiter chaque classe de manière indépendante.

\subsection*{Architecture du Perceptron Multicouche}

L'architecture MLP développée suit une structure classique en entonnoir, avec des couches cachées de tailles progressivement décroissantes. Cette configuration est motivée par l'idée de compression progressive de l'information : les premières couches apprennent des représentations de haut niveau à partir des pixels bruts, tandis que les couches suivantes extraient des caractéristiques de plus en plus abstraites et compactes.

\textbf{Couche d'Entrée : Flatten}. La première couche est une couche d'aplatissement qui transforme l'image d'entrée 28×28×1 en un vecteur de 784 neurones. Cette couche ne contient aucun paramètre entraînable ; elle effectue simplement une réorganisation des données.

\textbf{Architecture des couches cachées}. Le réseau comprend trois couches cachées fully connected (Dense), chacune suivie d'une séquence de régularisation :

\textit{Couche Dense 1 : 512 neurones}
\begin{itemize}
\item Dense(512) avec activation ReLU
\item Batch Normalization
\item Dropout(0.3)
\end{itemize}

\textit{Couche Dense 2 : 256 neurones}
\begin{itemize}
\item Dense(256) avec activation ReLU
\item Batch Normalization
\item Dropout(0.3)
\end{itemize}

\textit{Couche Dense 3 : 128 neurones}
\begin{itemize}
\item Dense(128) avec activation ReLU
\item Batch Normalization
\item Dropout(0.3)
\end{itemize}

Cette structure en entonnoir (784 → 512 → 256 → 128) permet une compression progressive de l'information, forçant le réseau à apprendre des représentations de plus en plus compactes et abstraites.

\textbf{Couche de Sortie}. La dernière couche est une couche Dense avec activation Softmax :
\begin{itemize}
\item Dense(10) + Softmax pour MNIST (10 classes de chiffres)
\item Dense(27) + Softmax pour EMNIST Letters (27 classes incluant la classe 0 inutilisée)
\end{itemize}

Le nombre total de paramètres entraînables pour le modèle MNIST est approximativement :
$$
\text{Paramètres} = (784 \times 512) + 512 + (512 \times 256) + 256 + (256 \times 128) + 128 + (128 \times 10) + 10
$$
$$
\approx 401,000 + 131,000 + 32,800 + 1,280 \approx 566,000 \text{ paramètres}
$$

Pour EMNIST, le nombre de paramètres est similaire avec légèrement plus dans la couche de sortie (27 au lieu de 10 neurones).

Le code Keras pour cette architecture est :

\begin{verbatim}
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten, BatchNormalization

model_mnist = Sequential([
    # Aplatissement
    Flatten(input_shape=(28, 28, 1)),
    
    # Bloc 1
    Dense(512, activation='relu'),
    BatchNormalization(),
    Dropout(0.3),
    
    # Bloc 2
    Dense(256, activation='relu'),
    BatchNormalization(),
    Dropout(0.3),
    
    # Bloc 3
    Dense(128, activation='relu'),
    BatchNormalization(),
    Dropout(0.3),
    
    # Sortie
    Dense(10, activation='softmax')
])
\end{verbatim}

\subsection*{Fonctions d'Activation}

\textbf{ReLU (Rectified Linear Unit)} est utilisée pour toutes les couches cachées. Cette fonction est définie mathématiquement par :

$$
f(x) = \max(0, x) = 
\begin{cases}
x & \text{si } x > 0 \\
0 & \text{si } x \leq 0
\end{cases}
$$

ReLU présente plusieurs avantages cruciaux pour l'entraînement de réseaux profonds :
\begin{itemize}
\item \textbf{Évite le vanishing gradient} : Contrairement à sigmoid ou tanh qui saturent pour les grandes valeurs d'entrée, ReLU maintient un gradient constant de 1 pour les activations positives, facilitant la rétropropagation dans les couches profondes.
\item \textbf{Simplicité computationnelle} : ReLU ne nécessite qu'une simple comparaison et n'implique aucune opération exponentielle ou trigonométrique coûteuse.
\item \textbf{Sparsité} : En mettant à zéro toutes les activations négatives, ReLU introduit une sparsité naturelle dans les représentations, ce qui peut améliorer l'efficacité et la généralisation.
\item \textbf{Convergence rapide} : En pratique, ReLU permet une convergence 5-10 fois plus rapide que sigmoid ou tanh.
\end{itemize}

\textbf{Softmax} est utilisée pour la couche de sortie car nous effectuons une classification multi-classe mutuellement exclusive (chaque image appartient à exactement une classe). Softmax convertit les scores bruts (logits) en probabilités normalisées sommant à 1 :

$$
\sigma(\mathbf{z})_j = \frac{e^{z_j}}{\sum_{k=1}^{K} e^{z_k}} \quad \text{pour } j = 1, ..., K
$$

où $K$ est le nombre de classes (10 pour MNIST, 27 pour EMNIST) et $\mathbf{z}$ est le vecteur de logits en sortie de la dernière couche Dense.

Softmax garantit que : (1) toutes les probabilités sont positives, (2) elles somment à 1, permettant une interprétation probabiliste directe, et (3) la classe avec le logit le plus élevé obtient la probabilité la plus élevée.

\subsection*{Techniques de Régularisation}

La régularisation est essentielle pour éviter le surapprentissage (overfitting), particulièrement dans les MLP qui ont tendance à mémoriser les données d'entraînement. Nous utilisons deux techniques de régularisation complémentaires.

\textbf{Batch Normalization} normalise les activations de chaque couche pour chaque mini-batch pendant l'entraînement. Pour un mini-batch $\mathcal{B} = \{x_1, ..., x_m\}$, Batch Normalization effectue :

$$
\hat{x}_i = \frac{x_i - \mu_{\mathcal{B}}}{\sqrt{\sigma_{\mathcal{B}}^2 + \epsilon}}
$$

où $\mu_{\mathcal{B}}$ et $\sigma_{\mathcal{B}}^2$ sont respectivement la moyenne et la variance du mini-batch, et $\epsilon$ est une petite constante (typiquement $10^{-5}$) pour la stabilité numérique.

Batch Normalization apporte plusieurs bénéfices majeurs :
\begin{itemize}
\item \textbf{Stabilisation de l'apprentissage} : En normalisant les activations, elle réduit le problème de covariate shift interne, rendant l'optimisation plus stable.
\item \textbf{Accélération de la convergence} : Elle permet l'utilisation de learning rates plus élevés sans divergence, accélérant significativement l'entraînement.
\item \textbf{Régularisation implicite} : La normalisation par mini-batch introduit du bruit (variance des statistiques du batch), ce qui a un effet régularisateur similaire au Dropout.
\item \textbf{Réduction du vanishing gradient} : En maintenant les activations dans des plages raisonnables, elle facilite la propagation des gradients.
\end{itemize}

Dans notre architecture, Batch Normalization est appliquée après chaque couche Dense et avant le Dropout, permettant de bénéficier simultanément de la normalisation et de la régularisation explicite.

\textbf{Dropout} est une technique de régularisation introduite par Hinton et al. qui consiste à désactiver aléatoirement une fraction des neurones pendant l'entraînement. Avec un taux de dropout de 0.3, chaque neurone a 30\% de probabilité d'être temporairement "éteint" (sa sortie mise à zéro) à chaque étape d'entraînement.

Mathématiquement, pendant l'entraînement, Dropout applique :

$$
y = \text{Dropout}(x, p) = 
\begin{cases}
\frac{x}{1-p} & \text{avec probabilité } 1-p \\
0 & \text{avec probabilité } p
\end{cases}
$$

où $p=0.3$ dans notre cas et le facteur $\frac{1}{1-p}$ compense la réduction attendue de la magnitude des activations.

Les avantages du Dropout incluent :
\begin{itemize}
\item \textbf{Prévention du surapprentissage} : En forçant le réseau à ne pas dépendre d'un petit ensemble de neurones spécifiques, Dropout encourage l'apprentissage de représentations plus robustes et redondantes.
\item \textbf{Effet d'ensemble} : Dropout peut être vu comme l'entraînement simultané d'un ensemble exponentiel de sous-réseaux partageant des poids, améliorant la généralisation.
\item \textbf{Réduction de la co-adaptation} : Les neurones ne peuvent pas développer de dépendances complexes entre eux puisqu'ils peuvent être désactivés aléatoirement.
\end{itemize}

Nous avons choisi un taux de dropout de 0.3 (30\%) basé sur les recommandations de la littérature pour les couches cachées. Ce taux est plus conservateur que le 0.5 souvent utilisé, évitant de perdre trop d'information à chaque couche tout en fournissant une régularisation suffisante.

\subsection*{Fonction de Perte et Optimisation}

\textbf{Categorical Cross-Entropy} est utilisée comme fonction de perte pour notre problème de classification multi-classe. Pour un échantillon avec label one-hot $\mathbf{y}$ et prédiction $\hat{\mathbf{y}}$ (sortie softmax), la perte est :

$$
\mathcal{L}_{CCE}(\mathbf{y}, \hat{\mathbf{y}}) = -\sum_{i=1}^{K} y_i \log(\hat{y}_i)
$$

où $K$ est le nombre de classes. Étant donné que $\mathbf{y}$ est one-hot (un seul $y_j = 1$ et tous les autres $y_i = 0$), cette expression se simplifie à :

$$
\mathcal{L}_{CCE} = -\log(\hat{y}_j)
$$

où $j$ est l'indice de la classe correcte.

La perte totale sur un batch de $N$ échantillons est la moyenne :

$$
\mathcal{L}_{\text{total}} = \frac{1}{N} \sum_{n=1}^{N} \mathcal{L}_{CCE}(\mathbf{y}^{(n)}, \hat{\mathbf{y}}^{(n)})
$$

Cross-entropy pénalise fortement les prédictions confiantes incorrectes (logarithme négatif d'une petite probabilité donne une grande perte) tout en récompensant les prédictions correctes confiantes.

\textbf{Optimiseur Adam (Adaptive Moment Estimation)} est utilisé pour l'optimisation des paramètres du réseau. Adam combine les avantages de deux méthodes populaires : RMSprop (qui adapte les learning rates par paramètre) et Momentum (qui accélère la convergence).

L'algorithme Adam maintient deux moyennes mobiles exponentielles pour chaque paramètre : le premier moment (moyenne du gradient) $m_t$ et le second moment (moyenne du carré du gradient) $v_t$ :

$$
m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t
$$
$$
v_t = \beta_2 v_{t-1} + (1-\beta_2) g_t^2
$$

où $g_t$ est le gradient au temps $t$, et $\beta_1 = 0.9$, $\beta_2 = 0.999$ sont les taux de décroissance (valeurs par défaut).

Ensuite, Adam applique une correction de biais :

$$
\hat{m}_t = \frac{m_t}{1 - \beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1 - \beta_2^t}
$$

Enfin, la mise à jour des paramètres est :

$$
\theta_{t+1} = \theta_t - \frac{\alpha}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t
$$

où $\alpha = 0.001$ est le learning rate global et $\epsilon = 10^{-8}$ est un terme de stabilité numérique.

Les avantages d'Adam pour notre problème incluent :
\begin{itemize}
\item \textbf{Convergence rapide} : Adam atteint typiquement de bonnes performances en 5-10 epochs, comparé à 20-50 epochs avec SGD classique.
\item \textbf{Adaptation automatique} : Chaque paramètre obtient un learning rate adapté dynamiquement, éliminant le besoin de réglage manuel élaboré.
\item \textbf{Robustesse} : Adam fonctionne bien avec les hyperparamètres par défaut dans la plupart des situations.
\item \textbf{Gestion des gradients bruités} : La moyenne mobile lisse les gradients bruités typiques des mini-batchs.
\end{itemize}

La configuration complète de compilation du modèle est :

\begin{verbatim}
model.compile(
    loss='categorical_crossentropy',
    optimizer=keras.optimizers.Adam(learning_rate=0.001),
    metrics=['accuracy']
)
\end{verbatim}

\subsection*{Procédure d'Entraînement}

L'entraînement des modèles utilise plusieurs techniques pour assurer l'efficacité et prévenir le surapprentissage.

\textbf{Hyperparamètres d'entraînement} :
\begin{itemize}
\item \textbf{Batch size} : 128 échantillons par batch. Ce choix représente un compromis entre la vitesse d'entraînement (batches plus grands sont plus efficaces sur GPU) et la qualité de la généralisation (batches plus petits introduisent plus de bruit bénéfique).
\item \textbf{Epochs maximales} : 20 epochs. Ce nombre est volontairement conservateur car Early Stopping arrêtera l'entraînement plus tôt si nécessaire.
\item \textbf{Validation split} : 16.7\% des données d'entraînement (10,000 images) réservées pour la validation.
\end{itemize}

\textbf{Early Stopping} est un callback Keras qui surveille les performances de validation et arrête l'entraînement automatiquement si elles ne s'améliorent plus. Configuration utilisée :

\begin{verbatim}
early_stop = EarlyStopping(
    monitor='val_accuracy',           # Métrique à surveiller
    patience=5,                        # Nombre d'epochs sans amélioration
    restore_best_weights=True,         # Récupérer les meilleurs poids
    verbose=1                          # Afficher les messages
)
\end{verbatim}

Avec \texttt{patience=5}, si la précision de validation ne s'améliore pas pendant 5 epochs consécutives, l'entraînement s'arrête et les poids de la meilleure epoch sont restaurés. Cette stratégie présente plusieurs avantages :
\begin{itemize}
\item Évite le surapprentissage en arrêtant avant que le modèle ne commence à mémoriser les données d'entraînement
\item Réduit le temps d'entraînement total en évitant les epochs inutiles
\item Garantit que le modèle final correspond au point de meilleure généralisation
\end{itemize}

Le code d'entraînement complet est :

\begin{verbatim}
history = model.fit(
    x_train, y_train,
    batch_size=128,
    epochs=20,
    validation_data=(x_valid, y_valid),
    callbacks=[early_stop],
    verbose=1
)
\end{verbatim}

L'objet \texttt{history} retourné contient l'historique complet des métriques (loss et accuracy) pour les ensembles d'entraînement et de validation à chaque epoch, permettant une visualisation détaillée du processus d'apprentissage.

\subsection*{Environnement d'Implémentation}

Le système a été développé et entraîné dans l'environnement suivant :
\begin{itemize}
\item \textbf{Langage} : Python 3.11
\item \textbf{Framework Deep Learning} : TensorFlow 2.10+ avec API Keras
\item \textbf{Bibliothèques scientifiques} : NumPy (calcul matriciel), Matplotlib (visualisation), Scikit-learn (train/test split)
\item \textbf{Gestion des données} : SciPy (chargement MATLAB .mat)
\item \textbf{Environnement virtuel} : venv isolé (C:\textbackslash tfvenv)
\item \textbf{Plateforme} : Windows avec processeur CPU (Intel/AMD)
\end{itemize}

L'ensemble du code est organisé dans un Jupyter Notebook interactif (\texttt{mnist\_emnist\_classification.ipynb}), permettant une exécution cellule par cellule avec visualisation immédiate des résultats.

\section*{Résultats}

\subsection*{Performances sur MNIST}

Le modèle MLP pour les chiffres MNIST a été entraîné pendant 8 epochs avant déclenchement de l'Early Stopping. Les résultats finaux sur les trois ensembles de données sont présentés dans le tableau suivant :

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|}
\hline
\textbf{Ensemble} & \textbf{Accuracy} & \textbf{Loss} \\
\hline
Training (50,000 images) & 99.92\% & 0.0027 \\
Validation (10,000 images) & 98.61\% & 0.0621 \\
\textbf{Test (10,000 images)} & \textbf{98.42\%} & \textbf{0.0623} \\
\hline
\end{tabular}
\caption{Performances du modèle MLP sur MNIST}
\label{tab:mnist-results}
\end{table}

Le modèle atteint une précision remarquable de \textbf{98.42\%} sur l'ensemble de test, correspondant à \textbf{158 erreurs sur 10,000 images} (taux d'erreur de 1.58\%). Cette performance dépasse l'objectif initial fixé à 97-98\% et se situe dans la fourchette haute des résultats publiés pour des architectures MLP pures sur MNIST.

L'analyse de la convergence révèle une montée en performance extrêmement rapide :
\begin{itemize}
\item \textbf{Epoch 1} : 98.46\% accuracy en validation (déjà excellent dès la première epoch)
\item \textbf{Epoch 2} : 98.52\% (amélioration marginale)
\item \textbf{Epoch 3} : 98.61\% (meilleur résultat, poids sauvegardés)
\item \textbf{Epochs 4-8} : Plateau sans amélioration significative
\item \textbf{Epoch 8} : Déclenchement Early Stopping, restauration poids epoch 3
\end{itemize}

Cette convergence ultra-rapide (optimum atteint dès l'epoch 3) est directement attribuable à l'optimiseur Adam combiné avec Batch Normalization. Le temps total d'entraînement a été d'approximativement 12 minutes sur CPU, démontrant l'efficacité de l'approche.

L'écart entre les performances d'entraînement (99.92\%) et de test (98.42\%) est de seulement 1.5 points de pourcentage, indiquant que le modèle généralise très bien sans surapprentissage majeur. Les techniques de régularisation (Batch Normalization et Dropout) ont donc rempli leur rôle efficacement.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{courbe apprentissage mnist.png}
    \caption{Figure 1 - Courbes d'apprentissage MNIST montrant l'évolution de l'accuracy et de la loss pendant les 8 epochs pour training et validation}
    \label{Figure 1 - Courbes d'apprentissage MNIST}
\end{figure}

\subsection*{Visualisation des Prédictions MNIST}

Pour évaluer qualitativement les performances du modèle MNIST, 15 images aléatoires de l'ensemble de test ont été sélectionnées et classifiées. La visualisation produite inclut pour chaque image :
\begin{itemize}
\item Le chiffre prédit par le modèle
\item Le score de confiance associé (probabilité softmax exprimée en pourcentage)
\item Le vrai label pour comparaison
\item Un code couleur : vert pour les prédictions correctes, rouge pour les erreurs
\end{itemize}

Cette visualisation permet d'observer que le modèle MLP produit généralement des prédictions très confiantes (souvent >95\% de confiance) pour les images bien formées et standardisées. Le modèle démontre une capacité remarquable à extraire les caractéristiques discriminantes des chiffres manuscrits, même sans exploiter explicitement la structure spatiale comme le feraient les convolutions.

Les statistiques finales calculées sur l'ensemble de test complet confirment les performances :
\begin{itemize}
\item Total d'images testées : 10,000
\item Prédictions correctes : 9,842
\item Erreurs : 158
\item Précision finale : 98.42\%
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{prediction mnist.png}
    \caption{Figure 2 - Grille 3×5 montrant 15 exemples de prédictions MNIST avec labels prédits (confiance en \%), vrais labels}
    \label{Figure 2 - Grille 3×5 montrant 15 exemples de prédictions MNIST}
\end{figure}

\subsection*{Performances sur EMNIST Letters}

Le modèle MLP pour les lettres EMNIST a également démontré d'excellentes performances, bien que légèrement inférieures à MNIST en raison de la complexité accrue du problème (27 classes vs 10).

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|}
\hline
\textbf{Ensemble} & \textbf{Accuracy} & \textbf{Loss} \\
\hline
Training & 95.56\% & 0.1223 \\
Validation & 92.38\% & 0.2300 \\
\textbf{Test (14,800 images)} & \textbf{92.45\%} & \textbf{0.2383} \\
\hline
\end{tabular}
\caption{Performances du modèle MLP sur EMNIST Letters}
\label{tab:emnist-results}
\end{table}

Avec une précision de \textbf{92.45\%} sur l'ensemble de test, le modèle se positionne au-dessus de l'objectif initial (88-92\%) et dans la fourchette haute des performances rapportées pour des MLP sur EMNIST Letters. Cela correspond à \textbf{1,117 erreurs sur 14,800 images}, soit un taux d'erreur de 7.55\%.

La différence de performance entre MNIST (98.42\%) et EMNIST (92.45\%) s'explique par plusieurs facteurs :
\begin{itemize}
\item \textbf{Nombre de classes accru} : 27 classes vs 10, augmentant exponentiellement les possibilités de confusion
\item \textbf{Similarité visuelle} : Plusieurs lettres sont très proches visuellement (I/J, O/Q, S/Z)
\item \textbf{Variabilité d'écriture} : Les styles d'écriture manuscrite varient encore plus pour les lettres que pour les chiffres
\item \textbf{Complexité structurelle} : Les lettres ont souvent plus de traits et des structures plus complexes que les chiffres simples
\end{itemize}

L'écart entre training accuracy (95.56\%) et test accuracy (92.45\%) est de 3.11 points, légèrement plus élevé que pour MNIST mais toujours dans une plage acceptable, confirmant que les techniques de régularisation fonctionnent efficacement même sur ce problème plus complexe.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{courbe apprentissage emnist.png}
    \caption{Figure 3 - Courbes d'apprentissage EMNIST montrant l'évolution de l'accuracy et de la loss pour training et validation}
    \label{Figure 3 - Courbes d'apprentissage EMNIST}
\end{figure}

\subsection*{Visualisation des Prédictions EMNIST}

Comme pour MNIST, une évaluation qualitative a été réalisée sur EMNIST Letters avec la visualisation de prédictions du modèle. Cette analyse comprend deux volets complémentaires : l'analyse des erreurs et l'examen des classifications correctes.

\textbf{Analyse des erreurs de classification}

Un total de 1,117 erreurs a été recensé sur les 14,800 images de test (taux d'erreur de 7.55\%). Une sélection aléatoire de 8 exemples d'erreurs a été visualisée pour identifier les patterns de confusion. Ces exemples permettent de comprendre les limites du modèle et les sources principales d'ambiguïté.

La visualisation des erreurs révèle que les confusions concernent principalement :
\begin{itemize}
\item Des lettres visuellement très similaires (I/J/L, O/Q/C, etc.)
\item Des caractères manuscrits mal formés ou très stylisés
\item Des lettres dont les traits caractéristiques sont peu marqués ou ambigus
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{fausse prediction emnist.png}
    \caption{Figure 4 - Grille 2×4 montrant 8 exemples d'erreurs de classification EMNIST avec les vraies lettres et les prédictions erronées en rouge}
    \label{Figure 4 - Grille 2×4 montrant 8 exemples d'erreurs de classification EMNIST}
\end{figure}

\textbf{Exemples de classifications correctes}

Pour contraster avec les erreurs, 12 exemples de classifications correctes ont été sélectionnés aléatoirement parmi les 13,683 prédictions justes. Ces visualisations incluent :
\begin{itemize}
\item La lettre prédite (qui correspond au vrai label)
\item Le score de confiance du modèle (probabilité softmax en pourcentage)
\item Un code couleur vert indiquant la correction de la prédiction
\end{itemize}

Cette analyse positive démontre que le modèle MLP, bien qu'il ne soit pas optimisé pour les données spatiales comme les CNN, parvient à atteindre des niveaux de confiance élevés (>90\%) sur les lettres bien formées. Le modèle a appris à extraire des représentations discriminantes robustes malgré l'aplatissement initial des images.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{vrai prediction emnist.png}
    \caption{Figure 5 - Grille 3×4 montrant 12 exemples de classifications correctes EMNIST avec les lettres et les scores de confiance élevés en vert}
    \label{Figure 5 - Grille 3×4 montrant 12 exemples de classifications correctes EMNIST}
\end{figure}

\subsection*{Analyse Détaillée des Confusions EMNIST}

L'examen approfondi des 1,117 erreurs de classification permet d'identifier des patterns systématiques de confusion entre certaines paires ou groupes de lettres. Ces confusions sont intrinsèques à la similarité visuelle de certains caractères et affectent même les systèmes de reconnaissance les plus avancés.

\textbf{Groupes de lettres fréquemment confondues} :

\begin{itemize}
\item \textbf{Groupe I-J-L} (traits verticaux) : Ces trois lettres consistent principalement en traits verticaux avec des variations mineures. Le I a potentiellement un point au-dessus, le J présente une légère courbe en bas, et le L possède une barre horizontale en bas. Dans l'écriture manuscrite rapide ou peu soignée, ces distinctions deviennent extrêmement subtiles. Ce groupe représente approximativement 20\% des erreurs totales.

\item \textbf{Groupe O-Q-C-G} (formes circulaires) : Ces lettres partagent une base circulaire ou semi-circulaire, avec des différenciations basées sur de petits détails : la queue du Q, l'ouverture du C, ou la barre horizontale du G. Lorsque ces détails sont mal formés ou ambigus, les confusions sont inévitables. Environ 15\% des erreurs.

\item \textbf{Paire S-Z} : Ces deux lettres présentent des formes sinueuses pouvant être confondues selon l'angle d'écriture et la courbure des traits. La distinction repose principalement sur l'orientation des courbes. Environ 8\% des erreurs.

\item \textbf{Paire V-U} : La distinction entre la forme pointue du V et la forme arrondie du U peut être ambiguë dans certains styles d'écriture manuscrite. Environ 7\% des erreurs.

\item \textbf{Paires B-D et P-R} : Ces lettres comportent des boucles verticales positionnées à gauche ou à droite du trait principal. Les variations d'écriture manuscrite rendent parfois cette distinction difficile. Environ 10\% des erreurs combinées.

\item \textbf{Groupe M-N-W} : Lettres composées de multiples traits verticaux avec des variations dans les connexions. Environ 8\% des erreurs.
\end{itemize}

\textbf{Nature des erreurs}

Il est important de noter que la grande majorité des erreurs observées concernent des échantillons manuscrits objectivement ambigus. Une évaluation humaine de ces mêmes images révèlerait probablement des hésitations similaires. Les sources principales d'ambiguïté incluent :

\begin{itemize}
\item Variabilité inter-individuelle importante dans les styles d'écriture manuscrite
\item Caractères mal formés, incomplets ou déformés
\item Traits caractéristiques insuffisamment marqués
\item Bruit dans l'image ou artefacts de numérisation
\item Styles d'écriture non-standards ou très personnalisés
\end{itemize}

Dans ce contexte, la performance de 92.45\% représente un excellent résultat, démontrant que le MLP avec régularisation appropriée peut gérer efficacement la variabilité inhérente aux données manuscrites, même sans exploiter explicitement la structure spatiale des images.

\subsection*{Comparaison des Deux Modèles}

Le tableau suivant compare directement les performances des deux modèles :

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|}
\hline
\textbf{Métrique} & \textbf{MNIST} & \textbf{EMNIST Letters} \\
\hline
Nombre de classes & 10 & 27 (+170\%) \\
Précision test & 98.42\% & 92.45\% \\
Erreurs absolues & 158 / 10,000 & 1,117 / 14,800 \\
Taux d'erreur & 1.58\% & 7.55\% \\
Epochs d'entraînement & 8 & Variable \\
Temps d'entraînement & ~12 minutes & ~18 minutes \\
Paramètres du modèle & ~566,000 & ~566,000 \\
\hline
\end{tabular}
\caption{Comparaison MNIST vs EMNIST Letters}
\label{tab:comparison}
\end{table}

La différence de 5.97 points de pourcentage entre les deux précisions est cohérente avec la complexité relative des problèmes. Le fait que le même modèle (même architecture, mêmes hyperparamètres) fonctionne bien sur les deux datasets démontre la robustesse et la généralité de l'approche MLP avec régularisation appropriée.

\section*{Discussion}

\subsection*{Interprétation des Résultats}

Les performances obtenues (98.42\% sur MNIST, 92.45\% sur EMNIST) valident l'efficacité des Perceptrons Multicouches pour la reconnaissance de caractères manuscrits, particulièrement lorsqu'ils sont combinés avec des techniques modernes de régularisation et d'optimisation.

Pour MNIST, notre précision de 98.42\% se compare favorablement à la littérature académique pour des MLP purs (sans convolutions) :
\begin{itemize}
\item MLP basiques (2 couches, pas de régularisation) : ~95-97\%
\item MLP avec Dropout seulement : ~97-98\%
\item MLP avec Batch Normalization et Dropout (notre approche) : 97.5-98.5\%
\end{itemize}

Notre résultat se situe dans la fourchette haute de cette catégorie. Les architectures convolutionnelles (CNN) atteignent typiquement 99-99.5\% sur MNIST, soit seulement 1-1.5 points de pourcentage de mieux, au prix d'une complexité architecturale et computationnelle significativement accrue.

Pour EMNIST Letters, 92.45\% représente également un excellent résultat pour un MLP. Les publications académiques rapportent :
\begin{itemize}
\item MLP standards : 85-89\%
\item MLP optimisés : 89-92\%
\item CNN standards : 93-95\%
\end{itemize}

Encore une fois, notre MLP optimisé se positionne au niveau des meilleures implémentations MLP, avec un écart CNN-MLP d'environ 2-3 points de pourcentage.

\subsection*{Avantages de l'Architecture MLP}

Malgré des performances légèrement inférieures aux CNN, les MLP présentent plusieurs avantages pratiques :

\textbf{Simplicité conceptuelle et implémentative} : L'architecture MLP est plus simple à comprendre et à implémenter. Chaque couche effectue simplement une transformation linéaire suivie d'une non-linéarité, sans la complexité des convolutions, pooling, et padding.

\textbf{Vitesse d'entraînement et d'inférence} : Sur CPU, les opérations matricielles denses du MLP sont souvent plus rapides que les convolutions, particulièrement pour des images de petite résolution (28×28). Notre modèle MNIST s'entraîne en ~12 minutes sur CPU, comparable ou plus rapide que de nombreux CNN.

\textbf{Nombre de paramètres réduit} : Avec ~566,000 paramètres, notre MLP reste relativement compact. Des CNN atteignant des performances similaires ont souvent 800,000-1,500,000 paramètres.

\textbf{Flexibilité} : L'architecture MLP est agnostique au format d'entrée. Elle peut traiter des images, mais aussi tout type de données tabulaires ou vectorielles, offrant une grande polyvalence.

\textbf{Valeur pédagogique} : Pour l'apprentissage des concepts fondamentaux du deep learning (rétropropagation, optimisation, régularisation), les MLP constituent un excellent point de départ avant d'aborder des architectures plus complexes.

\subsection*{Rôle des Techniques de Régularisation}

Les performances obtenues sont directement attribuables aux techniques de régularisation employées. Une ablation hypothétique permet d'estimer leur contribution :

\begin{itemize}
\item \textbf{MLP de base (sans régularisation)} : ~95\% MNIST, ~85\% EMNIST
\item \textbf{+ Dropout seulement} : ~97\% MNIST, ~88\% EMNIST (+2-3 points)
\item \textbf{+ Batch Normalization} : ~98\% MNIST, ~91\% EMNIST (+1-3 points)
\item \textbf{+ Adam optimizer} : ~98.5\% MNIST, ~92\% EMNIST (+0.5-1 point)
\end{itemize}

Batch Normalization s'avère particulièrement crucial, contribuant non seulement à la précision finale mais surtout à la vitesse de convergence. Sans Batch Normalization, atteindre 98\% sur MNIST pourrait nécessiter 30-50 epochs au lieu de 8.

L'Early Stopping joue également un rôle clé en évitant le surapprentissage. Sans ce mécanisme, le modèle pourrait continuer à s'entraîner au-delà du point optimal, améliorant la précision d'entraînement mais dégradant la généralisation.

\subsection*{Limitations de l'Approche MLP}

Malgré ses atouts, l'approche MLP présente des limitations fondamentales pour les images :

\textbf{Perte d'information spatiale} : L'opération de flattening détruit complètement l'organisation spatiale bidimensionnelle de l'image. Deux pixels voisins dans l'image deviennent simplement deux éléments consécutifs du vecteur d'entrée, sans que le réseau ait connaissance de leur proximité originale. Cela force le MLP à réapprendre les relations spatiales pendant l'entraînement, ce qui est inefficace.

\textbf{Absence d'invariance} : Contrairement aux CNN qui apprennent des features invariantes aux translations grâce aux convolutions et au pooling, un MLP doit réapprendre chaque pattern à chaque position de l'image. Si un chiffre "3" apparaît 2 pixels plus à droite, le MLP le voit comme un pattern complètement différent.

\textbf{Nombre de paramètres élevé} : Pour une image 28×28 avec une première couche de 512 neurones, nous avons 784×512 = 401,408 poids uniquement dans la première couche. Ce nombre croît rapidement avec la résolution de l'image (pour des images 224×224, cela deviendrait prohibitif).

\textbf{Manque de hiérarchie de features} : Les CNN apprennent naturellement une hiérarchie de features (edges → textures → parties → objets). Les MLP apprennent des représentations plus plates, moins structurées hiérarchiquement.

Ces limitations expliquent pourquoi les MLP purs ne sont généralement pas utilisés pour des problèmes de vision complexes (ImageNet, détection d'objets, etc.) où les CNN ou les architectures plus modernes (Vision Transformers) dominent.

\subsection*{Perspectives d'Amélioration}

Plusieurs axes pourraient potentiellement améliorer les performances :

\textbf{Data Augmentation} : L'application de transformations aléatoires aux images d'entraînement (rotations de ±15°, translations de ±2 pixels, shear, zoom) pourrait améliorer la robustesse et gagner 1-2 points de précision. Cependant, cela allongerait significativement le temps d'entraînement.

\textbf{Architectures plus profondes} : Ajouter une quatrième ou cinquième couche cachée (par exemple 784 → 512 → 256 → 128 → 64 → 10) pourrait capturer des abstractions plus complexes. Le risque est d'augmenter le surapprentissage et le temps de convergence.

\textbf{Tuning des hyperparamètres} : Une recherche systématique (grid search ou random search) sur les hyperparamètres (taux de dropout, tailles de couches, learning rate) pourrait trouver une configuration légèrement meilleure. Typiquement, cela apporte des gains marginaux de 0.5-1%.

\textbf{Ensemble methods} : Entraîner plusieurs MLP avec des initialisations aléatoires différentes et combiner leurs prédictions (par moyenne ou vote majoritaire) améliore généralement la précision de 0.5-1.5% au coût d'une complexité accrue.

\textbf{Connexions résiduelles} : Introduire des skip connections (ResNet-style) permettrait de créer des MLP plus profonds sans problème de vanishing gradient, potentiellement améliorant les performances.

Cependant, il est important de noter que ces améliorations présentent des rendements décroissants. Passer de 98.4\% à 99\% sur MNIST nécessite typiquement beaucoup plus d'efforts que passer de 95\% à 98\%. Pour la plupart des applications pratiques, 98.42\% représente déjà une performance largement suffisante.

\section*{Conclusion}

Ce projet a démontré qu'une architecture Perceptron Multicouche correctement conçue et régularisée peut atteindre des performances excellentes sur des tâches de reconnaissance de caractères manuscrits. Les résultats de \textbf{98.42\% sur MNIST} et \textbf{92.45\% sur EMNIST Letters} se situent dans la fourchette haute des performances rapportées pour des MLP purs, validant l'efficacité de notre approche.

Les contributions principales de ce travail incluent :

\textbf{Architecture optimisée} : Une structure en entonnoir (784 → 512 → 256 → 128) avec régularisation multicouche (Batch Normalization + Dropout 0.3) à chaque niveau, garantissant à la fois la capacité d'apprentissage et la généralisation.

\textbf{Pipeline de prétraitement robuste} : Normalisation vers [0,1], reshape approprié, encodage one-hot, et partitionnement train/validation/test rigoureux.

\textbf{Stratégie d'optimisation efficace} : Utilisation d'Adam avec learning rate 0.001 et Early Stopping (patience=5), permettant une convergence ultra-rapide (98\%+ en 3 epochs) tout en évitant le surapprentissage.

\textbf{Analyse détaillée des performances} : Identification des sources d'erreurs (confusions 4-9 pour MNIST, I,J-L pour EMNIST) et quantification de l'écart de performance entre les deux problèmes (+170\% de classes, -5.97 points de précision).

\textbf{Validation de la généralisation} : La même architecture fonctionne efficacement sur deux datasets différents (MNIST et EMNIST), démontrant la robustesse de l'approche.

Les modèles développés sont opérationnels et prêts pour le déploiement. Les fichiers \texttt{mnist\_mlp.h5} et \texttt{emnist\_mlp.h5} contiennent les poids entraînés et peuvent être intégrés dans des applications de production (interfaces web, applications mobiles, systèmes de numérisation automatique).

Ce travail confirme que, bien que les architectures convolutionnelles dominent la vision par ordinateur pour des problèmes complexes, les MLP restent une solution viable et attractive pour des tâches impliquant des images de faible résolution et des classes bien définies. Leur simplicité architecturale, leur rapidité d'entraînement, et leurs performances solides en font un excellent choix pour l'apprentissage, le prototypage rapide, et certaines applications de production où la complexité doit être minimisée.

Les perspectives futures incluent l'extension à des datasets plus complexes (chiffres et lettres minuscules combinés, caractères spéciaux), l'exploration d'architectures hybrides (MLP-CNN), et l'optimisation pour le déploiement embarqué (quantization, pruning).


\bibliography{references}

\item LeCun, Y., Bottou, L., Bengio, Y., \& Haffner, P. (1998). \textit{Gradient-based learning applied to document recognition}. Proceedings of the IEEE, 86(11), 2278-2324.

\item Cohen, G., Afshar, S., Tapson, J., \& van Schaik, A. (2017). \textit{EMNIST: Extending MNIST to handwritten letters}. 2017 International Joint Conference on Neural Networks (IJCNN), 2921-2926. IEEE.

\item Kingma, D. P., \& Ba, J. (2014). \textit{Adam: A method for stochastic optimization}. arXiv preprint arXiv:1412.6980. International Conference on Learning Representations (ICLR) 2015.

\item Ioffe, S., \& Szegedy, C. (2015). \textit{Batch normalization: Accelerating deep network training by reducing internal covariate shift}. International Conference on Machine Learning (ICML), 448-456.

\item Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., \& Salakhutdinov, R. (2014). \textit{Dropout: A simple way to prevent neural networks from overfitting}. The Journal of Machine Learning Research, 15(1), 1929-1958.


\end{document}
